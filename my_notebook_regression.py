# -*- coding: utf-8 -*-
"""My_Notebook_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wBSI56I55X9wcv98gqoobb5RXkmswt6Q
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import pandas
import missingno
import seaborn as sns

from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

def cargar_archivo(archivo):
  dataframe=pandas.read_csv(archivo)
  return dataframe

df_test=cargar_archivo('test.csv')
df_train=cargar_archivo('train.csv')

#df_sample=cargar_archivo('sample_submission.csv')

#df_test=df_test.merge(df_sample,how='inner',on='PassengerId')

def validación_balanceo(dataframe):
  columnas=['HomePlanet','CryoSleep','Destination','VIP','Transported']
  for columna in columnas:
    agrupado=dataframe.get(['PassengerId',columna]).groupby(columna).count()
    print(agrupado)
validación_balanceo(df_train)

def viz_nan(dataframe):
  missingno.matrix(dataframe)
  print('Los nulos por columna son:')
  print(dataframe.isnull().sum())
  dataframe=dataframe.dropna(subset=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck'])
  return dataframe
df_train_clean=viz_nan(df_train)

df_train_clean = pandas.get_dummies(df_train_clean, columns=['Destination'], prefix='', prefix_sep='')
print(df_train_clean)

def visualizacion(dataframe):
  sns.pairplot(dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']], diag_kind='kde')
visualizacion(df_train_clean)

def separated_train_validate(dataframe):
  train_dataframe = dataframe.sample(frac=0.8, random_state=0)
  validate_dataframe = dataframe.drop(train_dataframe.index)
  return train_dataframe,validate_dataframe

train_dataframe,validate_dataframe=separated_train_validate(df_train_clean)

train_features = train_dataframe.copy()
validate_features = validate_dataframe.copy()
#test_features = df_test.copy()

train_labels = train_features.pop('Transported')
validate_labels = validate_features.pop('Transported')
#test_labels = test_features.pop('Transported')

#train_features = train_dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']]
#validate_features = validate_dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']]

columns=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']

train_features = train_features[columns]

escalar = StandardScaler()
train_features = escalar.fit_transform(train_features)  ## Encontrar la media y varianza
validate_features = escalar.transform(validate_features[columns])

model = LogisticRegression()

model.fit(train_features, train_labels)

validate_pred = model.predict(validate_features)

#Verifico la matriz de Confusión
print(validate_features.shape)
print(validate_labels.shape)

matriz = confusion_matrix(validate_labels, validate_pred)
print('Matriz de Confusión:')
print(matriz)

precision = precision_score(validate_labels, validate_pred)
print('Precisión del modelo:')
print(precision)

exactitud = accuracy_score(validate_labels, validate_pred)
print('Exactitud del modelo:')
print(exactitud)

import pickle
with open('modelo_regresion_logistica.pkl', 'wb') as file:
    pickle.dump(model, file)

with open('escalador.pkl', 'wb') as file:
    pickle.dump(escalar, file)