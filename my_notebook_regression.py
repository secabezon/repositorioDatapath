# -*- coding: utf-8 -*-
"""My_Notebook_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wBSI56I55X9wcv98gqoobb5RXkmswt6Q
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import pandas
import missingno
import seaborn as sns

from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

def cargar_archivo(archivo):
  dataframe=pandas.read_csv(archivo)
  return dataframe

df_train=cargar_archivo('train.csv')

def validación_balanceo(dataframe):
  columnas=['HomePlanet','CryoSleep','Destination','VIP','Transported']
  for columna in columnas:
    agrupado=dataframe.get(['PassengerId',columna]).groupby(columna).count()
    print(agrupado)
validación_balanceo(df_train)

def viz_nan(dataframe):
  missingno.matrix(dataframe)
  print('Los nulos por columna son:')
  print(dataframe.isnull().sum())
  dataframe=dataframe.dropna(subset=['Age','Destination','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck'])
  return dataframe
df_train_clean=viz_nan(df_train)

df_train_clean = pandas.get_dummies(df_train_clean, columns=['Destination'], prefix='', prefix_sep='')
print(df_train_clean)

def visualizacion(dataframe):
  sns.pairplot(dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']], diag_kind='kde')
visualizacion(df_train_clean)

def separated_train_validate(dataframe):
  train_dataframe = dataframe.sample(frac=0.8, random_state=0)
  test_dataframe = dataframe.drop(train_dataframe.index)
  validate_dataframe = train_dataframe.sample(frac=0.2, random_state=0)
  train_dataframe = train_dataframe.drop(validate_dataframe.index)
  return train_dataframe,validate_dataframe,test_dataframe

train_dataframe,validate_dataframe,test_dataframe=separated_train_validate(df_train_clean)

train_features = train_dataframe.copy()
validate_features = validate_dataframe.copy()
test_features = test_dataframe.copy()

train_labels = train_features.pop('Transported')
validate_labels = validate_features.pop('Transported')
test_labels = test_features.pop('Transported')

columns=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']

train_features = train_features[columns]

escalar = StandardScaler()
train_features = escalar.fit_transform(train_features)  ## Encontrar la media y varianza
validate_features = escalar.transform(validate_features[columns])
test_features = escalar.transform(test_features[columns])

model = LogisticRegression(max_iter=60)

model.fit(train_features, train_labels)

validate_pred = model.predict(validate_features)

matriz = confusion_matrix(validate_labels, validate_pred)
print('Matriz de Confusión:')
print(matriz)

precision = precision_score(validate_labels, validate_pred)
print('Precisión del modelo:')
print(precision)

exactitud = accuracy_score(validate_labels, validate_pred)
print('Exactitud del modelo:')
print(exactitud)

test_pred = model.predict(test_features)

matriz = confusion_matrix(test_labels, test_pred)
print('Matriz de Confusión:')
print(matriz)

precision = precision_score(test_labels, test_pred)
print('Precisión del modelo:')
print(precision)

exactitud = accuracy_score(test_labels, test_pred)
print('Exactitud del modelo:')
print(exactitud)

df_test_example_predict=cargar_archivo('test.csv')
df_test_example_predict=df_test_example_predict.dropna(subset=['Age','Destination','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck'])
df_test_example_predict = pandas.get_dummies(df_test_example_predict, columns=['Destination'], prefix='', prefix_sep='')
test_example_predict = escalar.transform(df_test_example_predict[columns])

test_example_pred = model.predict(test_example_predict)
print(test_example_pred)

import pickle
with open('modelo_regresion_logistica.pkl', 'wb') as file:
    pickle.dump(model, file)

with open('escalador.pkl', 'wb') as file:
    pickle.dump(escalar, file)