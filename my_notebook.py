# -*- coding: utf-8 -*-
"""My_Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U9zXzmMZgWIxKODhr9OW4oIhYbqx-I__
"""

import seaborn as sns
import pandas
import missingno
import tensorflow as tf
from tensorflow import keras
import numpy
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pickle

def cargar_archivo(archivo):
  dataframe=pandas.read_csv(archivo)
  return dataframe

df_test=cargar_archivo('test.csv')
df_train=cargar_archivo('train.csv')
df_sample=cargar_archivo('sample_submission.csv')

#df_test=df_test.merge(df_sample,how='inner',on='PassengerId')

def validación_balanceo(dataframe):
  columnas=['HomePlanet','CryoSleep','Destination','VIP','Transported']
  for columna in columnas:
    agrupado=dataframe.get(['PassengerId',columna]).groupby(columna).count()
    print(agrupado)
validación_balanceo(df_train)

def viz_nan(dataframe):
  missingno.matrix(dataframe)
  print('Los nulos por columna son:')
  print(dataframe.isnull().sum())
  dataframe=dataframe.dropna(subset=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck'])
  return dataframe
df_train_clean=viz_nan(df_train)

df_train_clean = pandas.get_dummies(df_train_clean, columns=['Destination'], prefix='', prefix_sep='')
print(df_train_clean)

def visualizacion(dataframe):
  sns.pairplot(dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']], diag_kind='kde')
visualizacion(df_train_clean)

def separated_train_validate(dataframe):
  train_dataframe = dataframe.sample(frac=0.8, random_state=0)
  validate_dataframe = dataframe.drop(train_dataframe.index)
  return train_dataframe,validate_dataframe

train_dataframe,validate_dataframe=separated_train_validate(df_train_clean)

train_features = train_dataframe.copy()
validate_features = validate_dataframe.copy()
#test_features = df_test.copy()

train_labels = train_features.pop('Transported')
validate_labels = validate_features.pop('Transported')
#test_labels = test_features.pop('Transported')

#train_features = train_dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']]
#validate_features = validate_dataframe[['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']]

columns=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TRAPPIST-1e','PSO J318.5-22','55 Cancri e']

train_features = train_features[columns]

normalizer = tf.keras.layers.Normalization(axis=-1)

import cloudpickle

with open('normalizador.pkl', 'wb') as handle:
    cloudpickle.dump(normalizer, handle)

normalizer.adapt(numpy.array(train_features))

print(normalizer.mean.numpy())
print(normalizer.variance.numpy())

features = numpy.array(train_features)

features_normalizer = layers.Normalization(input_shape=[9,], axis=None)

features_normalizer.adapt(features)

model = keras.Sequential([
    features_normalizer,
    layers.Dense(32, activation='relu'),
    layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(2, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

NO_EPOCHS = 50
BATCH_SIZE = 128

train_model = model.fit(train_features, train_labels,
                  batch_size=BATCH_SIZE,
                  epochs=NO_EPOCHS,
                  verbose=1,
                  validation_data=(validate_features[columns], validate_labels))

test_loss, test_acc = model.evaluate(validate_features[columns], validate_labels, verbose=2)

print('\nTest accuracy:', test_acc)

def plot_loss(history):
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.ylim([0.45, 0.49])
    plt.xlabel('Epoch')
    plt.ylabel('Error [Transported]')
    plt.legend()
    plt.grid(True)
plot_loss(train_model)

model.save('Modelo_redes_neuronales.h5')

# Obtener las bibliotecas instaladas
!pip freeze > requirements.txt

# Descargar el archivo requirements.txt
from google.colab import files
files.download('requirements.txt')